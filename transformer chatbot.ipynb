{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a chatbot using transformers from scratch using the Pytorch library.\n",
    "Pytorch already has a transformer class as well as transformer encoders and decoders. However here we will code it from scratch to help understand the concept.\n",
    "\n",
    "We will be using the Cornell Movie Dialogs Corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_movie_conv='data/cornell movie-dialogs corpus/movie_conversations.txt'\n",
    "corpus_movie_lines = 'data/cornell movie-dialogs corpus/movie_lines.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing\n",
    "1. Load the data in batches to feed into the network. To store this data we need to find max length and pad the shorter sentences.\n",
    "\n",
    "2. These datasets tell us the conversations that happen, it includes which lines are in a conversation, the character that said each line, etc. Extract the line number and utterance. \n",
    "\n",
    "3. Format the conversations into a dictonary.\n",
    "\n",
    "4. Group conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=25\n",
    "\n",
    "# conversations\n",
    "with open(corpus_movie_conv,'r') as c:\n",
    "    conv = c.readlines()\n",
    "\n",
    "# lines\n",
    "with open(corpus_movie_lines,'r') as l:\n",
    "    lines = l.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 ', ' u0 ', ' m0 ', ' BIANCA ', ' They do not!\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conv\n",
    "#lines\n",
    "\n",
    "lines[0]\n",
    "# we want to extract line id and the saying \n",
    "lines[0].split('+++$+++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict={}\n",
    "for line in lines:\n",
    "    objects = line.split(' +++$+++ ')\n",
    "    lines_dict[objects[0]] = objects[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(string):\n",
    "    '''Remove punctuation to make it easier for NN'''\n",
    "    punctuations='''!()-[]{};:'\"\\,<>./?@#$%^&*_`~'''\n",
    "    no_punc=''\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punc+=char\n",
    "    return no_punc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for con in conv:\n",
    "    ids=eval(con.split(' +++$+++ ')[-1])\n",
    "    # note eval converts string to list and strip function removes extra spacing in a string\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "        \n",
    "        if i ==len(ids)-1:\n",
    "            break\n",
    "            \n",
    "        first =remove_punc(lines_dict[ids[i]].strip())\n",
    "        second =remove_punc(lines_dict[ids[i+1]].strip())\n",
    "        \n",
    "        # Create a 2D list\n",
    "        # Append and split strings because you want to process words one at a time in the transformer\n",
    "        # Trim to maximum length so we can assemble in a matrix\n",
    "        qa_pairs.append(first.split()[:max_len])\n",
    "        qa_pairs.append(second.split()[:max_len])\n",
    "        \n",
    "        # take the whole 2D list and append to the pairs list.\n",
    "        pairs.append(qa_pairs)\n",
    "        \n",
    "        \n",
    "# eval(conv[0].split(' +++$+++ ')[-1])\n",
    "# lines_dict['L194'].strip().split()\n",
    "# len(pairs)\n",
    "# pairs[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find word frequency for all conversation pairs.\n",
    "word_freq = Counter()\n",
    "for pair in pairs:\n",
    "    # this only updates unique words\n",
    "    word_freq.update(pair[0])\n",
    "    word_freq.update(pair[1])\n",
    "    \n",
    "# add minimum word frequency so vocabulary doesn't get out of hand. \n",
    "min_word_freq = 5\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "\n",
    "# construct word to index dictionary and pytorch will map it to a OHE which will be mapped to embedding\n",
    "word_map = {k:v+1 for v,k in enumerate(words)}\n",
    "\n",
    "# add tokens\n",
    "# unknown will be for all words with a frequency of 5 or less\n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "word_map['<start>'] = len(word_map) + 1\n",
    "word_map['<end>'] = len(word_map) + 1\n",
    "# index for padding will be zero. Therefore when checking if words in a sentence !=0 the non padded words will be 1 and padded\n",
    "# words will be 0\n",
    "word_map['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words are 18243\n"
     ]
    }
   ],
   "source": [
    "print('Total words are {}'.format(len(word_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump so we dont need to run the code again we can directly load it\n",
    "with open('WORDMAP_corpus.json','w') as j:\n",
    "    json.dump(word_map,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we cant provide words to NN so we will provide indices\n",
    "# note the get function will get the value of a key and if its not present it will get value of unknown token\n",
    "# add padding for sententences less than max_len\n",
    "\n",
    "def encode_question(words, word_map):\n",
    "    enc_c = [word_map.get(word,word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    return enc_c\n",
    "\n",
    "# pairs[0][0]\n",
    "# encode_question(pairs[0][0],word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we need to include start and end token when decoding.\n",
    "def encode_reply(words, word_map):\n",
    "    enc_c = [word_map['<start>']] + [word_map.get(word,word_map['<unk>']) for word in words] + [word_map['<end>']] + ([word_map['<pad>']] * (max_len - len(words)))\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will encode the pairs\n",
    "pairs_encoded=[]\n",
    "\n",
    "for pair in pairs:\n",
    "    ques = encode_question(pair[0],word_map)\n",
    "    ans = encode_reply(pair[1],word_map)\n",
    "    \n",
    "    #2D list of encoded pairs\n",
    "    pairs_encoded.append([ques, ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pairs encoded.\n",
    "with open('pairs_encoded.json','w') as w:\n",
    "    json.dump(pairs_encoded,w)\n",
    "# now we dont need to run the code again we can directly load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset that will inherit from pytorch Dataset class so it has all of its functions and attributes\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,path='pairs_encoded.json'):\n",
    "        # define loading function\n",
    "        self.pairs = json.load(open(path))\n",
    "        self.dataset_size = len(self.pairs)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        '''We will override the getitem function from dataset class. This function is what retreives a sample from the \n",
    "        dataset. This will retrieve one element and loop for self.dataset_size.\n",
    "        Note: needs to be long tensor because these are discrete integer values.'''\n",
    "        \n",
    "        question = torch.LongTensor(self.pairs[i][0])\n",
    "        reply = torch.LongTensor(self.pairs[i][1])\n",
    "        \n",
    "        # return one pair, later in dataloader these will be returned in batches. \n",
    "        return question, reply\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader to load data in batches. \n",
    "train_loader = torch.utils.data.DataLoader(Dataset(), \n",
    "                                          batch_size = 100,\n",
    "                                          shuffle = True,\n",
    "                                         pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets visualize a batch of samples. \n",
    "question, reply = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 25])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.shape\n",
    "# 100 samples, 25 words is maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 27])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply.shape\n",
    "# 27 also includes the start and end token so 2 extra words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create input and target masks for the decoder.\n",
    "\n",
    "def create_masks(question, reply_input, reply_target):\n",
    "    \n",
    "    #input example:\n",
    "    #sentence: <start> I slept last night <end>\n",
    "    # reply input: <start> I slept last night\n",
    "    # reply target: I slept last night <end>\n",
    "    \n",
    "    \n",
    "    def subsequent_mask(size):\n",
    "        '''creates the whole matrix mask for one sentence'''\n",
    "        # takes in size of mask\n",
    "        # note when we use masks we want them to be integers\n",
    "        #pytorch has a function that does the transpose of masking already\n",
    "        # mask needs to be integer type not float.\n",
    "        mask = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8)\n",
    "        # since 4D tensors we need to unsqueezed in 0th dimention to add a dimention\n",
    "        return mask.unsqueeze(0)\n",
    "    \n",
    "    # We need a question mask so words are 1 and padded elements are 0.\n",
    "    # Also the question mask is just one line (batchsize=1 when you do that). try typing question[0]\n",
    "    question_mask = (question!=0).to(device)\n",
    "    #unsqueeze because we are using 4D tensor so mask should also be 4D, need to add 2Ds here\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)  #(batchsize,1,1,maxwords)\n",
    "    \n",
    "    # do same for the reply input mask. This is to put a 1 where there are words and 0 where there is padding.\n",
    "    reply_input_mask = reply_input!=0\n",
    "    # Just like question mask again, it needs to be unsqueezed twice to make it a 4D mask, however second time needs \n",
    "    # to be done after its been combined with subsequent mask because now decoder masks both padded AND future words\n",
    "    # we want decoder to only attend to words already generated\n",
    "    reply_input_mask=reply_input_mask.unsqueeze(1)   #(batchsize,1,maxwords)\n",
    "    reply_input_mask=reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data)  #-1 is maxwords\n",
    "    # that gave (batchsize,maxwords,maxwords)\n",
    "    reply_input_mask=reply_input_mask.unsqueeze(1) \n",
    "    \n",
    "    # this doesnt need to be unsqueezed because not used in model just loss \n",
    "    reply_target_mask=reply_target!=0\n",
    "    \n",
    "    return question_mask, reply_input_mask, reply_target_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''responsible for word embeddings for input sequence and positional encoding'''\n",
    "class Embeddings(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,d_model,max_len=50):\n",
    "        #d_model is dimentionality of embeddings, and I know max len is 25 but we will trim this later\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model=d_model\n",
    "        # we will use dropout as regularization, we will follow hyperparameters in the paper\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size,d_model)  # input is size of the dictionary of embeddings and output is size of\n",
    "                                                        # the desired embedding vectors\n",
    "        \n",
    "        # positional encoding (pe) is fixed matrix with no params to upadate so no backprop through this matrix\n",
    "        self.pe = self.create_positional_encoding(max_len,d_model)\n",
    "        \n",
    "    def create_positional_encoding(self,max_len,d_model):\n",
    "        # recall max_len is x axis, and d_model is how many curves you create to make positional vector\n",
    "        # recall must be same shape as embedding so can be added together.\n",
    "        pe = torch.zeros(max_len,d_model).to(device)\n",
    "        \n",
    "        for pos in range(max_len):\n",
    "            for i in range(0,d_model,2):\n",
    "                # for each position we include sin and cos elements up until total dimantions of model\n",
    "                pe[pos,i] = math.sin(pos/(10000 **((2*i)/d_model)))\n",
    "                pe[pos,i+1] = math.cos(pos/(10000 **((2*(i+1))/d_model)))\n",
    "                # pos will go to maximum length\n",
    "                \n",
    "        # we are working in batches so unsqueeze\n",
    "        # 1 will be automatically expanded with same batch size as encoded words in forward function\n",
    "        pe = pe.unsqueeze(0)  # 0 because batch size is in first dimention (1,max_len,d_model)\n",
    "        \n",
    "        return pe\n",
    "    \n",
    "    def forward(self,encoded_words):\n",
    "        '''Tell pytorch how to run the class'''\n",
    "        \n",
    "        # recall we want to give more wieght to words rather than positional encodings so mult\n",
    "        # by sqrt of dimentionality of model\n",
    "        embeddings = self.embed(encoded_words) * math.sqrt(self.d_model) #(batchsize,max_words,d_model)\n",
    "        \n",
    "        #trim positional encoding by max words extracted from embeddings\n",
    "#         max_words=embeddings.size[1]\n",
    "        embeddings += self.pe[:,:embeddings.size(1)] # pe will automatically be expandied to batch size of embeddings matrix\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall 3 kinds of attention in transformer`\n",
    "# encoder self attention\n",
    "# decoder masked self attention\n",
    "# decoder source attention from encoder\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,heads,d_model):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        # need to ensure number of heads is compatible with dimentionality of embeddings \n",
    "        # assert - make sure statement is correct\n",
    "        assert d_model % heads == 0\n",
    "        \n",
    "        # dimentionality of each head\n",
    "        self.d_k = d_model // heads # division result with no remainder\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # from embeddings we create three weights matricies \n",
    "        self.query = nn.Linear(d_model,d_model)\n",
    "        self.key = nn.Linear(d_model,d_model)\n",
    "        self.value = nn.Linear(d_model,d_model)\n",
    "        \n",
    "        # concat layer that allows concat head results to interact\n",
    "        self.concat = nn.Linear(d_model,d_model)\n",
    "        \n",
    "    def forward(self,query,key,value,mask):\n",
    "            '''# in self attention and masked self attention all 3 come from model input, in source attention the \n",
    "            # value comes from encoded representation from encoder \n",
    "            query, key and value: (batchsize,max_words,512)\n",
    "            mask for masked self attention, mask for source attention, masked self multihead attention \n",
    "            in decoder (batchsize,1,1,max_words)'''\n",
    "            \n",
    "            query = self.query(query)  #(batchsize,maxwords,512)\n",
    "            key = self.key(key)        #(batchsize,maxwords,512)\n",
    "            value = self.value(value)  #(batchsize,maxwords,512)\n",
    "            \n",
    "            #reshape by number of heads\n",
    "            # note cant do (-1)view trick in 1 step because it will reshape the max_words to number of heads (8), \n",
    "            # which you don't want\n",
    "            # (batchsize,maxwords,512) --> (batchsize,maxwords,8,64) --> (batchsize,8,maxwords,64)\n",
    "            query = query.view(query.shape[0],-1,self.heads,self.d_k).permute(0,2,1,3)  \n",
    "            key = key.view(query.shape[0],-1,self.heads,self.d_k).permute(0,2,1,3)  \n",
    "            value = value.view(query.shape[0],-1,self.heads,self.d_k).permute(0,2,1,3)  \n",
    "            \n",
    "            #dot Q and K\n",
    "            # (batchsize,8,maxwords,64) dot (batchsize,8,maxwords,64)T --> (batchsize,8,maxwords,maxwords)\n",
    "            # note in source attention first maxwords is max words from decoder and second is maxwords from encoder\n",
    "            # in self attention theyre the same\n",
    "            # when you do dot product or transpose on 4D matrix the first 2Ds stay the same. \n",
    "            scores = torch.matmul(query,key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
    "            # mask the padded elements using masked_full method from pytorch. you can pass a mask\n",
    "            # and specify a value you want replaced. we will replace with small value so its ignored\n",
    "            # by softmax to prevent calculation of attention weights over padded elements\n",
    "            scores = scores.masked_fill(mask ==0,1e-9)\n",
    "            # pass through softmax over all words - always over -1 \n",
    "            weights = F.softmax(scores,dim=-1)\n",
    "            weights = self.dropout(weights)\n",
    "            \n",
    "            # dot product with value\n",
    "            #(batchsize,8,maxwords,maxwords) dot (batchsize,8,maxwords,64)\n",
    "            context = torch.matmul(weights,value)\n",
    "            \n",
    "            # concat context matrix into a vector for all heads together\n",
    "            # (batchsize,8,maxwords,64) --> (batchsize,maxwords,8,64)--> (batchsize,maxwords,8*64)\n",
    "            context = context.permute(0,2,1,3).reshape(context.shape[0],-1,self.heads*self.d_k)\n",
    "            \n",
    "            # run though linear interaction layer\n",
    "            interacted=self.concat(context)\n",
    "            return interacted  #(batchsize,maxwords,8*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model,middle_dim=2048):\n",
    "        super(FeedForward,self).__init__()\n",
    "        \n",
    "        self.fc1=nn.Linear(d_model,middle_dim)\n",
    "        self.fc2=nn.Linear(middle_dim,d_model)\n",
    "        self.dropout=nn.Dropout(0,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x is output of multihead attention\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out  #(batchsize,maxwords,8*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_model,heads):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        \n",
    "        #in encoder we only have self attention - not source attention\n",
    "        # its also not masked\n",
    "        self.self_multihead = MultiHeadAttention(heads,d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        #layernorm is applied to output of attention and FF which both have dimantionality of 512\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        \n",
    "    def forward(self,embeddings, mask):\n",
    "        '''mask is source mask which I believe stops the model from attending to padded tokens, we can see forward \n",
    "        function of multihead class takes query key and values but those are all just the embeddings in the \n",
    "        case of embedding'''\n",
    "        interacted = self.self_multihead(embeddings,embeddings,embeddings,mask)\n",
    "        interacted = self.dropout(interacted)\n",
    "        #residual connection\n",
    "        interacted = interacted + embeddings\n",
    "        #layer norm\n",
    "        interacted = self.layernorm(interacted)\n",
    "        \n",
    "        #FF\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        #residual connection\n",
    "        feed_forward_out = feed_forward_out + interacted \n",
    "        #layer norm\n",
    "        encoded = self.layernorm(feed_forward_out)\n",
    "        \n",
    "        # encoded representation of 1 block\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word embeddings needed for the masked self attention, encoded representation from encoder needed for\n",
    "#source attention,source mask needed for encoded representation, target mask for \n",
    "#self attention so can attend to its own inpstart with self attention - get query from decoder\n",
    "# we can see forward function of multihead class takes query key and values but those areall just \n",
    "#the embeddings\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    # same as encoder but need to add source attention which attends to encoder outputs\n",
    "    def __init__(self,d_model,heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.self_multihead = MultiHeadAttention(heads,d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads,d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        #layernorm is applied to output of attention and FF which both have dimantionality of 512\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self,embeddings,encoded,src_mask, target_mask):\n",
    "        query = self.self_multihead(embeddings,embeddings,embeddings,target_mask)\n",
    "        query = self.dropout(query)\n",
    "        \n",
    "        \n",
    "        #layer norm and residule connections\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        \n",
    "        # now we need to apply source multi head attention that attends to the encoder output\n",
    "        # the query comes from self attention of decoder, keys and values come from encoded representation\n",
    "        interacted = self.src_multihead(query,encoded,encoded,src_mask)\n",
    "        interacted = self.dropout(interacted)\n",
    "        \n",
    "        #layer norm and residule connections\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        \n",
    "        #feed forward\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        \n",
    "        return decoded \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads, num_layers, word_map):\n",
    "        super(Transformer,self).__init__()\n",
    "        # num_layers is number of encoder layers, all have predefined architecture we specified but\n",
    "        # will have different weights\n",
    "        self.d_model = d_model\n",
    "        self.vocab = len(word_map)\n",
    "        self.embed = Embeddings(self.vocab, d_model)\n",
    "        \n",
    "        #assemble models in a list, this is list of 6 encoder layers\n",
    "        # note underscore is more memory efficient, \n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        \n",
    "        #classification layer\n",
    "        #input will be size d_model which is size of decoded that is returned in decoder layer\n",
    "        self.logit = nn.Linear(d_model,self.vocab)\n",
    "        \n",
    "    def encode(self,src_words,src_mask):\n",
    "        #src_words is question and src_mask is the question mask\n",
    "        src_embeddings = self.embed(src_words)\n",
    "        \n",
    "        # for each encoder layer supply updated parameter inputs from output of previous layer\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings, src_mask)\n",
    "        return src_embeddings\n",
    "    \n",
    "    def decode(self,target_words,target_mask, src_embeddings,src_mask):\n",
    "        # src mask prevents attention over padded words\n",
    "        # target mask is mask for self attention to rpevent peeking\n",
    "        \n",
    "        # use same embeddings because question and reply share the same vocab - different for translation tasks\n",
    "        tgt_embeddings = self.embed(target_words)\n",
    "        \n",
    "        # for each encoder layer supply updated parameter inputs from output of previous layer\n",
    "        for layer in self.decoder:\n",
    "            tgt_embeddings = layer(tgt_embeddings,src_embeddings, src_mask, target_mask)\n",
    "        return tgt_embeddings\n",
    "    \n",
    "    def forward(self,src_words,src_mask,target_words,target_mask):\n",
    "        encoded = self.encode(src_words,src_mask)\n",
    "        decoded = self.decode(target_words,target_mask,encoded,src_mask)\n",
    "        out = self.logit(decoded)\n",
    "        # log softmax doesnt need to be used if we use nn.crossentorpyloss because it will automatically take log and softmax\n",
    "        #output, but here were usng KL divergence loss so we need to specify it manually. \n",
    "        out = F.log_softmax(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "    '''Create an instance of this to train/update weights'''\n",
    "    def __init__(self,model_size,warmup_steps,optimizer):\n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        '''create adamwarmup equation'''\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "    \n",
    "    def step(self):\n",
    "        '''called each time you update lr and will update the weights\n",
    "        recall the normal step function optimizer.step() which does w<--w-lr*gradient.'''\n",
    "        \n",
    "        self.current_step +=1\n",
    "        lr=self.get_lr()\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            #params group is a dictionary that includes the lr which we will update\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        self.lr=lr\n",
    "        \n",
    "        #update weight\n",
    "        self.optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWithLS(nn.Module):\n",
    "    def __init__(self, size, smooth):\n",
    "        #size is vocab size and smooth is smoothing param\n",
    "        super(LossWithLS,self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average = False, reduce = False)\n",
    "        #size avg =true would average over all samples however we manually compute avg by number of words in the mask. In the\n",
    "        # mask 1 means you have a word so we will sum to get num of words and divide by this\n",
    "        # reduce =True would change the shape to make 1D vector which we dont want because we want to multiply predictions\n",
    "        # with the mask later on and the mask has a specific shape we want to follow. \n",
    "        # to get average the words correspond to a value of 1 so we will sum up all of mask. \n",
    "        self.confidence = 1 - smooth\n",
    "        self.smooth = smooth\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, prediction, target, mask):\n",
    "        '''takes the prediction, target, and mask for the target \n",
    "         prediction comes out of logit layer in decoder --> (batchsize,max_words,vocab_size)\n",
    "            target mask comes from create_mask function which output question mask, reply_input_mask (utilized \n",
    "            in transformer), and reply_target_mask (utilized in loss function)\n",
    "            target and mask have shape (batch_size,max_words)'''\n",
    "            \n",
    "        # do reshaping because we are processing words in parallel rather than in sequence:\n",
    "        #reshape prediction (batchsize,maxwords,vocabsize) --> (batchsize*maxwords,vocabsize)\n",
    "        prediction = prediction.view(-1,prediction.size(-1))\n",
    "        # reshape target and mask (batchsize,maxwords) --> (batchsize*maxwords)\n",
    "        # note target is indicies of correct vocab so values go up to vocabsize\n",
    "        # print(target.size())\n",
    "        target = target.reshape(-1)\n",
    "        mask = mask.float()\n",
    "        # note target and mask have the same shape\n",
    "        mask = mask.reshape(-1)\n",
    "            \n",
    "        #labels will be smoothed version of target variable\n",
    "        # copy predictions to get label variable in proper shape which will be smoothed \n",
    "        labels = prediction.data.clone()\n",
    "        #replace data\n",
    "        labels.fill_(self.smooth / self.size-1)\n",
    "        # place confidence value in index of correct class\n",
    "        # scatter function will create labels in which we want to minimize KL div\n",
    "        # input first dimention(vocab size),  the indicies of the target, and with smoothing\n",
    "        # also note target needs to be same size as labels in scatter so unsqueeze\n",
    "        labels.scatter(1,target.data.unsqueeze(1),self.confidence)\n",
    "        # now labels have smoothed based on the true target to minimize KL divergence with prediction\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = self.criterion(prediction,labels)   #(batchsize*maxwords,vocabsize) shape is retained\n",
    "        # because we specified False in instance of KL div\n",
    "        \n",
    "        # take sum of loss over dim=1(vocab) to get 1D tensor, mask is same shape as 1D tensor \n",
    "        # so can be multiplied to mask out the padded words. sum losses for non padded words. Get\n",
    "        # average loss of non padded words by dividing by number of non padded words\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=512\n",
    "heads = 8\n",
    "num_layers= 1  # in paper it is 6\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 1  # in paper its 25\n",
    "\n",
    "with open('WORDMAP_corpus.json','r') as j:\n",
    "    word_map = json.load(j)\n",
    "    \n",
    "    \n",
    "transformer = Transformer(d_model=d_model, heads = heads, num_layers = num_layers, word_map = word_map)\n",
    "transformer.to(device)\n",
    "\n",
    "# parameters is all weights, define lr as 0 because adam warmup will change it and set it\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(),lr=0, betas = (0.9, 0.98), eps = 1e-9)\n",
    "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "\n",
    "#size is vocab size aka length of wordmap\n",
    "criterion = LossWithLS(size = len(word_map), smooth = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, transformer, criterion, epoch):\n",
    "    '''recieves train_laoder which is in charge of loading in batches, the model which is the transformer, \n",
    "    criterion which is loss function, and epoch number'''\n",
    "    \n",
    "    # specify training mode because we used batchnorm which acts differently in train and test time\n",
    "    transformer.train()\n",
    "    sum_loss = 0\n",
    "    #take incriment according to number of samles\n",
    "    count = 0\n",
    "    \n",
    "    # we can see from train loader function that it returns question and reply\n",
    "    for i, (question, reply) in enumerate(train_loader):\n",
    "        \n",
    "        #get batch size\n",
    "        samples = question.shape[0]\n",
    "        \n",
    "        #move to device\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "        \n",
    "        # prepare encoder input, take whole batch and everything in maxwords except last word which is end token\n",
    "        reply_input = reply[:,:-1]\n",
    "        #dont take start token\n",
    "        reply_target = reply[:,1:]\n",
    "        #print(reply_target.size())\n",
    "        #eg sentence: <start> I went home <end>\n",
    "        # input: <start> I went home\n",
    "        # target aka decoder prediction: I went home <end>\n",
    "        \n",
    "        # create masks\n",
    "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
    "        \n",
    "        # run inputs and masks through transformer\n",
    "        out = transformer(question,question_mask,reply_input,reply_input_mask)\n",
    "        \n",
    "        #calculate loss\n",
    "        loss= criterion(out,reply_target, reply_target_mask)\n",
    "        \n",
    "        #backprop\n",
    "        # zero out gradient at each new batch\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        #calcualate gradient\n",
    "        loss.backward()\n",
    "        #update weight\n",
    "        transformer_optimizer.step()\n",
    "        \n",
    "        #calculate statistics\n",
    "        sum_loss+=loss.item()*samples  # only get value not whole tensor. At last batch for an epoch the count = len(train_loader)*100\n",
    "        count += samples\n",
    "        if i% 100 == 0:\n",
    "            print('Epoch [{}][{}]/[{}]\\tLoss: {:.3f}'.format(epoch,i,len(train_loader),sum_loss/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
    "    '''Performs greedy search decoding with batch size of 1.'''\n",
    "    # output of transformer is indicies so we want to do opposite of wordmap\n",
    "    rev_word_map = {v:k for k,v in word_map.items()}\n",
    "    \n",
    "    transformer.eval()\n",
    "    # create start token to tell decoder to start decoding, get from wordmap\n",
    "    start_token = word_map['<start>']\n",
    "    # get encoded representation\n",
    "    encoded = transformer.encode(question,question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device)  #must be 2D matrix so 2 brackets. word -->(1,1)\n",
    "    \n",
    "    #concat generated words frm decoder to next input of decoder\n",
    "    # note first token is start token\n",
    "    for step in range(max_len - 1):\n",
    "        size = words.shape[0]  # in first iteration will be 1 then will increase\n",
    "        \n",
    "        #create target mask, cant use create mask funtion because size is varying eg size starts with just 1 word\n",
    "        # so for each step impliment new target mask\n",
    "        target_mask = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8)\n",
    "        # need to unsqueeze because we need it in 4D\n",
    "        target_mask = target_mask.to(device).unsqueeze(0)\n",
    "        \n",
    "        # decode \n",
    "        decoded = transformer.decode(words,target_mask, encoded,question_mask)\n",
    "        #output layer\n",
    "        \n",
    "        #decoded is shape (batchsize=1,maxwords=1,vocabsize)\n",
    "        # take only last element, we only want prediction of last word so we want to take final decoded output\n",
    "        predictions = transformer.logit(decoded[:,-1])\n",
    "        # predictions is shape (1,vocabsize)\n",
    "        #predict next word by taking max of output\n",
    "        _, next_word = torch.max(predictions,dim=1)  # next_word -->(1,1)\n",
    "        #since we called torch it returns a tensor, extract the value\n",
    "        next_word = next_word.item()\n",
    "        \n",
    "        #check to see if were at the end\n",
    "        if next_word == word_map['<end>']:\n",
    "            break\n",
    "            \n",
    "        words = torch.cat([words,torch.LongTensor([[next_word]]).to(device)], dim=1) # shape is now (1,step+2) because on dim=1\n",
    "        \n",
    "        # at end we have rpedicted words concat together\n",
    "        #turn into sentence by getting rid of the 0th dimention, also in target we dont want start token\n",
    "    words = words.squeeze(0)\n",
    "    words = words.tolist()\n",
    "    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n",
    "    #convert indices to words #convert to string\n",
    "    sentence = \" \".join(rev_word_map[sen_idx[k]] for k in range(len(sen_idx)))\n",
    "        \n",
    "    return sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 26])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-147-5b8425e520e2>:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.log_softmax(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][0]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][100]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][200]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][300]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][400]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][500]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][600]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][700]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][800]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][900]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][1000]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][1100]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][1200]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][1300]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][1400]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][1500]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][1600]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][1700]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][1800]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][1900]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][2000]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][2100]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "Epoch [0][2200]/[2217]\tLoss: 0.000\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([100, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "torch.Size([16, 26])\n",
      "tensor(0., grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, transformer, criterion, epoch)\n",
    "    #save checkpoint(saving a dictionary) - model weights, optimizer, and model, and epoch we're at\n",
    "    state = {'epoch':epoch, 'transformer':transformer, \n",
    "             'transformer_optimizer':transformer_optimizer}\n",
    "    torch.save(state, 'checkpoint_'+str(epoch)+'.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how are you\n",
      "Enter max words to be generated: 3\n",
      "ups utoldu\n",
      "Question: what is the weather?\n",
      "Enter max words to be generated: 5\n",
      "fruitcake tales eliminate —\n"
     ]
    }
   ],
   "source": [
    "# function for evaluation on users own input\n",
    "# load checkpoint so if you cleared the kernel and ran this it owuld load the checkpoints with the\n",
    "# weights and transformer model\n",
    "# usually include epoch number, we will train it for 1 epoch so put a zero\n",
    "checkpoint = torch.load('checkpoint_0.tar')\n",
    "#load model and its weights\n",
    "transformer = checkpoint['transformer']\n",
    "\n",
    "#perform eval- go until user quits\n",
    "while(1):\n",
    "    #get user input\n",
    "    question = input(\"Question: \")\n",
    "    if question == 'quit':\n",
    "        break\n",
    "    max_len = input(\"Enter max words to be generated: \")\n",
    "    \n",
    "    #encode questions using wordmap\n",
    "    enc_question =[word_map.get(word,word_map['<unk>']) for word in question.split()] \n",
    "    #transform to long tensor and get index of each word\n",
    "    question = torch.LongTensor(enc_question).to(device).unsqueeze(0)\n",
    "    #create mask for all non padded words (aka not equal to zero)\n",
    "    #unsqueeze twice because 4D\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)\n",
    "    #evaluate function to return sentence as a string\n",
    "    sentence = evaluate(transformer,question,question_mask,int(max_len),word_map)\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
